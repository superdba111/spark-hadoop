{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc6e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous profile: de\n",
      "Setting new profile to: de\n"
     ]
    }
   ],
   "source": [
    "%profile \"de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "649b8079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous region: us-east-1\n",
      "Setting new region to: us-east-1\n",
      "Reauthenticating Glue client with new region: us-east-1\n",
      "IAM role has been set to arn:aws:iam::605516946663:role/LakeFormationWorkflowRole. Reauthenticating.\n",
      "Authenticating with profile=de\n",
      "glue_role_arn defined by user: arn:aws:iam::605516946663:role/LakeFormationWorkflowRole\n",
      "Authentication done.\n",
      "Region is set to: us-east-1\n"
     ]
    }
   ],
   "source": [
    "%region \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a7a37bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Glue version to: 3.0\n"
     ]
    }
   ],
   "source": [
    "%glue_version 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b40363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous number of workers: 2\n",
      "Setting new number of workers to: 2\n"
     ]
    }
   ],
   "source": [
    "%number_of_workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eea6eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iam_role is arn:aws:iam::605516946663:role/LakeFormationWorkflowRole\n",
      "iam_role has been set to arn:aws:iam::605516946663:role/LakeFormationWorkflowRole.\n"
     ]
    }
   ],
   "source": [
    "%iam_role arn:aws:iam::605516946663:role/LakeFormationWorkflowRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b7b217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333ec1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "# PYSPARK\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F \n",
    "spark=SparkSession.builder.appName('spark_session').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379be4b8",
   "metadata": {},
   "source": [
    "Creating Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871180b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS\n",
    "df_pandas = [['A1', 'B1', 2, '21-12-2021 10:30'], \n",
    "      ['A2', 'B2', 4, '21-12-2021 10:40'], \n",
    "      ['A3', 'B3', 5, '21-12-2021 11:00']] # Rows\n",
    "df_pandas = pd.DataFrame(df_pandas, columns = ['A', 'B', 'Value', 'Date_Column'])\n",
    "# PYSPARK\n",
    "df_pyspark = spark.createDataFrame(\n",
    "    [('A1', 'B1', 2, '21-12-2021 10:30'), \n",
    "     ('A2', 'B2', 4, '21-12-2021 10:40'), \n",
    "     ('A3', 'B3', 5, '21-12-2021 11:00')], # Rows\n",
    "    ['A', 'B', 'Value', 'Date_Column'] # Columns\n",
    ")\n",
    "# NOTE: There are multiple ways of creating dataframes in both the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9eb6863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B  Value       Date_Column\n",
      "0  A1  B1      2  21-12-2021 10:30\n",
      "1  A2  B2      4  21-12-2021 10:40\n",
      "2  A3  B3      5  21-12-2021 11:00\n"
     ]
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4626201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: string (nullable = true)\n",
      " |-- B: string (nullable = true)\n",
      " |-- Value: long (nullable = true)\n",
      " |-- Date_Column: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03829f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------------+\n",
      "|  A|  B|Value|     Date_Column|\n",
      "+---+---+-----+----------------+\n",
      "| A1| B1|    2|21-12-2021 10:30|\n",
      "| A2| B2|    4|21-12-2021 10:40|\n",
      "| A3| B3|    5|21-12-2021 11:00|\n",
      "+---+---+-----+----------------+\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12645b",
   "metadata": {},
   "source": [
    "Creating New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19cd67d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - New column with Constant values\n",
    "df_pandas['C'] = 'New Constant'\n",
    "# PYSPARK - New column with Constant values\n",
    "df_pyspark = df_pyspark.withColumn(\"C\", F.lit('New Constant'))\n",
    "# PANDAS - New Column using existing columns\n",
    "df_pandas['C'] = df_pandas['A']+df_pandas['B']\n",
    "# PYSPARK - New Column using existing columns\n",
    "df_pyspark = df_pyspark.withColumn(\"C\", F.concat(\"A\", \"B\"))\n",
    "# NOTE:\n",
    "# lit() -- used to create constant columns\n",
    "# concat() -- concatenate columns of dataframe\n",
    "# withColumn() -- creates a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f1d054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B  Value       Date_Column     C\n",
      "0  A1  B1      2  21-12-2021 10:30  A1B1\n",
      "1  A2  B2      4  21-12-2021 10:40  A2B2\n",
      "2  A3  B3      5  21-12-2021 11:00  A3B3\n"
     ]
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be597f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------------+----+\n",
      "|  A|  B|Value|     Date_Column|   C|\n",
      "+---+---+-----+----------------+----+\n",
      "| A1| B1|    2|21-12-2021 10:30|A1B1|\n",
      "| A2| B2|    4|21-12-2021 10:40|A2B2|\n",
      "| A3| B3|    5|21-12-2021 11:00|A3B3|\n",
      "+---+---+-----+----------------+----+\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a66226",
   "metadata": {},
   "source": [
    "Updating Existing Column Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe2fdd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - Update Column data\n",
    "df_pandas['Value'] = df_pandas['Value']**2\n",
    "# PYSPARK - Update Column data\n",
    "df_pyspark = df_pyspark.withColumn(\"Value\", F.col(\"Value\")**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "129ba247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B  Value       Date_Column     C\n",
      "0  A1  B1      4  21-12-2021 10:30  A1B1\n",
      "1  A2  B2     16  21-12-2021 10:40  A2B2\n",
      "2  A3  B3     25  21-12-2021 11:00  A3B3\n"
     ]
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb1b2e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------------+----+\n",
      "|  A|  B|Value|     Date_Column|   C|\n",
      "+---+---+-----+----------------+----+\n",
      "| A1| B1|  4.0|21-12-2021 10:30|A1B1|\n",
      "| A2| B2| 16.0|21-12-2021 10:40|A2B2|\n",
      "| A3| B3| 25.0|21-12-2021 11:00|A3B3|\n",
      "+---+---+-----+----------------+----+\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91352f1",
   "metadata": {},
   "source": [
    "Selecting, Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "008a810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - Selecting Columns\n",
    "new_df_pandas = df_pandas[['B', 'C']]\n",
    "# PYSPARK - Selecting Columns\n",
    "new_df_pyspark = df_pyspark.select(\"B\", \"C\")\n",
    "\n",
    "# PANDAS - Filtering rows based on condition\n",
    "new_df_pandas = df_pandas[df_pandas['Value']<5]\n",
    "# PYSPARK - Filtering rows based on condition\n",
    "new_df_pyspark = df_pyspark.filter(df_pyspark.Value<5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "664cce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B  Value       Date_Column     C\n",
      "0  A1  B1      4  21-12-2021 10:30  A1B1\n"
     ]
    }
   ],
   "source": [
    "new_df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "881dc675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------------+----+\n",
      "|  A|  B|Value|     Date_Column|   C|\n",
      "+---+---+-----+----------------+----+\n",
      "| A1| B1|  4.0|21-12-2021 10:30|A1B1|\n",
      "+---+---+-----+----------------+----+\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b870a32",
   "metadata": {},
   "source": [
    "Column Type Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d590a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - Convert Column from String to DateTime format\n",
    "df_pandas['Date_Column'] =  pd.to_datetime(df_pandas['Date_Column'], format='%d-%m-%Y %H:%M')\n",
    "# PYSPARK - Convert Column from String to Timestamp format\n",
    "df_pyspark = df_pyspark.withColumn(\"Date_Column\", F.to_timestamp(\"Date_Column\", \"dd-MM-yyyy hh:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58fd96b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B  Value         Date_Column     C\n",
      "0  A1  B1      4 2021-12-21 10:30:00  A1B1\n",
      "1  A2  B2     16 2021-12-21 10:40:00  A2B2\n",
      "2  A3  B3     25 2021-12-21 11:00:00  A3B3\n"
     ]
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fee3e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-------------------+----+\n",
      "|  A|  B|Value|        Date_Column|   C|\n",
      "+---+---+-----+-------------------+----+\n",
      "| A1| B1|  4.0|2021-12-21 10:30:00|A1B1|\n",
      "| A2| B2| 16.0|2021-12-21 10:40:00|A2B2|\n",
      "| A3| B3| 25.0|2021-12-21 11:00:00|A3B3|\n",
      "+---+---+-----+-------------------+----+\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393aa822",
   "metadata": {},
   "source": [
    "Rename, Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f589a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - Rename Columns\n",
    "df_pandas = df_pandas.rename(columns={'A': 'Col_A', 'B': 'Col_B'})\n",
    "# PYSPARK - Rename Columns\n",
    "df_pyspark = df_pyspark.withColumnRenamed(\"A\", \"Col_A\").withColumnRenamed(\"B\", \"Col_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11cf4ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Col_A Col_B  Value         Date_Column     C\n",
      "0    A1    B1      4 2021-12-21 10:30:00  A1B1\n",
      "1    A2    B2     16 2021-12-21 10:40:00  A2B2\n",
      "2    A3    B3     25 2021-12-21 11:00:00  A3B3\n"
     ]
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "226b9f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-------------------+----+\n",
      "|Col_A|Col_B|Value|        Date_Column|   C|\n",
      "+-----+-----+-----+-------------------+----+\n",
      "|   A1|   B1|  4.0|2021-12-21 10:30:00|A1B1|\n",
      "|   A2|   B2| 16.0|2021-12-21 10:40:00|A2B2|\n",
      "|   A3|   B3| 25.0|2021-12-21 11:00:00|A3B3|\n",
      "+-----+-----+-----+-------------------+----+\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f37dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: \"['Col_A' 'Col_B'] not found in axis\"\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - Drop Columns\n",
    "df_pandas = df_pandas.drop(['Col_A', 'Col_B'], axis=1)\n",
    "# PYSPARK - Drop Columns\n",
    "df_pyspark = df_pyspark.drop('Col_A', 'Col_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f5a807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Value         Date_Column     C\n",
      "0      4 2021-12-21 10:30:00  A1B1\n",
      "1     16 2021-12-21 10:40:00  A2B2\n",
      "2     25 2021-12-21 11:00:00  A3B3\n"
     ]
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05630c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-------------------+----+\n",
      "|Col_A|Col_B|Value|        Date_Column|   C|\n",
      "+-----+-----+-----+-------------------+----+\n",
      "|   A1|   B1|  4.0|2021-12-21 10:30:00|A1B1|\n",
      "|   A2|   B2| 16.0|2021-12-21 10:40:00|A2B2|\n",
      "|   A3|   B3| 25.0|2021-12-21 11:00:00|A3B3|\n",
      "+-----+-----+-----+-------------------+----+\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f393ca87",
   "metadata": {},
   "source": [
    "Melt Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8dbd090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS\n",
    "df1 = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n",
    "                   'B': {0: 1, 1: 3, 2: 5},\n",
    "                   'C': {0: 2, 1: 4, 2: 6}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff6f0064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C\n",
      "0  a  1  2\n",
      "1  b  3  4\n",
      "2  c  5  6\n"
     ]
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e72aa5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A variable  value\n",
      "0  a        B      1\n",
      "1  b        B      3\n",
      "2  c        B      5\n",
      "3  a        C      2\n",
      "4  b        C      4\n",
      "5  c        C      6\n"
     ]
    }
   ],
   "source": [
    "pd.melt(df1, id_vars=['A'], value_vars=['B', 'C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d885d75",
   "metadata": {},
   "source": [
    "There is no direct version of pd.melt available in Pyspark (Release 3.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e442f9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PYSPARK custom melt function\n",
    "def melt(df, \n",
    "         id_vars, \n",
    "         value_vars, \n",
    "         var_name=\"Variable\", \n",
    "         value_name=\"Value\"):\n",
    "    _vars_and_vals = F.array(*(F.struct(F.lit(c).alias(var_name),\n",
    "                   F.col(c).alias(value_name)) for c in value_vars))\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\",\n",
    "                         F.explode(_vars_and_vals))\n",
    "    cols = id_vars + [\n",
    "            F.col(\"_vars_and_vals\")[x].alias(x) for x in [var_name,\n",
    "            value_name]]\n",
    "    return _tmp.select(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2e12b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "|  A|Variable|Value|\n",
      "+---+--------+-----+\n",
      "|  a|       B|    1|\n",
      "|  a|       C|    2|\n",
      "|  b|       B|    3|\n",
      "|  b|       C|    4|\n",
      "|  c|       B|    5|\n",
      "|  c|       C|    6|\n",
      "+---+--------+-----+\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [('a', 1, 2), ('b', 3, 4), ('c', 5, 6)], # Rows\n",
    "    ['A', 'B', 'C'] # Columns\n",
    ")\n",
    "melt(df, ['A'], ['B', 'C']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3a405",
   "metadata": {},
   "source": [
    "Add Interval to a Timestamp Column (Timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9086c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - Add 'Interval' to 'Start_Time'\n",
    "df2 = pd.DataFrame([['2021-01-10 10:10:00', '00:05'],\n",
    "                   ['2021-12-10, 05:30:00', '00:15'],\n",
    "                   ['2021-11-10 11:40:00', '00:20']], \n",
    "                   columns = ['Start_Time','Interval'])\n",
    "df2['Start_Time'] = pd.to_datetime(df2['Start_Time'])\n",
    "df2['End_Time'] = df2['Start_Time'] + \\\n",
    "                 pd.to_timedelta(pd.to_datetime(df2['Interval']).dt.\\\n",
    "                 strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18b13a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Start_Time Interval            End_Time\n",
      "0 2021-01-10 10:10:00    00:05 2021-01-10 10:15:00\n",
      "1 2021-12-10 05:30:00    00:15 2021-12-10 05:45:00\n",
      "2 2021-11-10 11:40:00    00:20 2021-11-10 12:00:00\n"
     ]
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8060b4fe",
   "metadata": {},
   "source": [
    "There is no inbuilt method similar to “to_timedelta” in Pyspark till date. This is an alternate way of doing it in Pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5acddd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PYSPARK - Add 'Interval' to 'Start_Time'\n",
    "df = spark.createDataFrame(\n",
    "    [['2021-01-10 10:10:00', '00:05'], \n",
    "     ['2021-12-10 05:30:00', '00:15'], \n",
    "     ['2021-11-10 11:40:00', '00:20']], # Rows\n",
    "     ['Start_Time', 'Interval'] # Columns\n",
    ")\n",
    "df = df.withColumn(\"Start_Time\", \n",
    "                   F.to_timestamp(\"Start_Time\", \n",
    "                                  \"yyyy-MM-dd hh:mm:ss\"))\n",
    "df = df.withColumn(\"End_Time\", (F.unix_timestamp(\"Start_Time\") + F.unix_timestamp(\"Interval\", \"HH:mm\")).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "113d1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------------------+\n",
      "|         Start_Time|Interval|           End_Time|\n",
      "+-------------------+--------+-------------------+\n",
      "|2021-01-10 10:10:00|   00:05|2021-01-10 10:15:00|\n",
      "|2021-12-10 05:30:00|   00:15|2021-12-10 05:45:00|\n",
      "|2021-11-10 11:40:00|   00:20|2021-11-10 12:00:00|\n",
      "+-------------------+--------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3b77b",
   "metadata": {},
   "source": [
    "other syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81fd7465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PANDAS df\n",
    "df11 = pd.DataFrame({'A': {0: 'a', 1: 'a', 2: 'c'},\n",
    "                   'B': {0: 1, 1: 1, 2: 5},\n",
    "                   'C': {0: 2, 1: 4, 2: 6}})\n",
    "# PYSPARK df\n",
    "df22 = spark.createDataFrame(\n",
    "    [('a', 1, 2), ('a', 1, 4), ('c', 5, 6)], # Rows\n",
    "    ['A', 'B', 'C'] # Columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62107d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - Shape of dataframe\n",
    "print(df11.shape)\n",
    "# PYSPARK - Shape of dataframe\n",
    "print((df22.count(), len(df22.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "facb4b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[A: string]\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - Distinct Values of a Column\n",
    "df11['A'].unique()\n",
    "# PYSPARK - Distinct Values of a Column\n",
    "df22.select('A').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "971eaa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     C\n",
      "A B   \n",
      "a 1  6\n",
      "c 5  6\n"
     ]
    }
   ],
   "source": [
    "# PANDAS - Group by Columns - Calculate Aggregate functions\n",
    "df11.groupby(['A', 'B']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0164a58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|  A|  B|sum(C)|\n",
      "+---+---+------+\n",
      "|  a|  1|     6|\n",
      "|  c|  5|     6|\n",
      "+---+---+------+\n"
     ]
    }
   ],
   "source": [
    "# PYSPARK - Group by Columns - Calculate Aggregate functions\n",
    "df22.groupBy(\"A\", \"B\").agg(F.sum(\"C\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf4f72",
   "metadata": {},
   "source": [
    "While Pandas is still favourite choice of many data scientists, one can comfortably use it for smaller datasets which do not consume too much memory. Pyspark can be used for larger datasets where distributed computing is used at its best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
