{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "k-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. The MLlib implementation includes a parallelized variant of the k-means++ method called kmeans||.\n",
    "\n",
    "KMeans is implemented as an Estimator and generates a KMeansModel as the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "Cluster Centers: \n",
      "[0.1 0.1 0.1]\n",
      "[9.1 9.1 9.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MLib basic example2\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"D:\\spark231\\data\\mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet allocation (LDA)\n",
    "LDA is implemented as an Estimator that supports both EMLDAOptimizer and OnlineLDAOptimizer, and generates a LDAModel as the base model. Expert users may cast a LDAModel generated by EMLDAOptimizer to a DistributedLDAModel if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -814.5453825970202\n",
      "The upper bound on perplexity: 3.1328668262402797\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[2, 6, 3]  |[0.10135346378618618, 0.1002967414125139, 0.09735247633239014] |\n",
      "|1    |[7, 2, 1]  |[0.10312032718733208, 0.10174491388853585, 0.09735074367228563]|\n",
      "|2    |[2, 1, 7]  |[0.10075729309275268, 0.09979360539720367, 0.09948326105599117]|\n",
      "|3    |[2, 8, 7]  |[0.11213850734514816, 0.09740255259887654, 0.095742150453585]  |\n",
      "|4    |[3, 6, 0]  |[0.10657612094563403, 0.1030644589993423, 0.10285952473296472] |\n",
      "|5    |[6, 10, 3] |[0.2018571350551996, 0.1996928309202472, 0.13944373426417167]  |\n",
      "|6    |[0, 10, 2] |[0.10648495826196677, 0.10393447996247314, 0.10049510479984035]|\n",
      "|7    |[2, 7, 8]  |[0.10620248856320555, 0.10324094816933772, 0.09771919374930409]|\n",
      "|8    |[4, 1, 3]  |[0.1648699605195974, 0.13614600318351086, 0.12528515339291346] |\n",
      "|9    |[8, 1, 10] |[0.10587757580584917, 0.10330193337551788, 0.09945981429917541]|\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n",
      "+-----+---------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                      |\n",
      "+-----+---------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.0047349565770060525,0.00473494968579037,0.004734917529367783,0.004735003716757925,0.004734777847838305,0.3962312598681093,0.004734951037129467,0.004735008238467582,0.5658892939433904,0.0047348815561429275]       |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.007899322785705867,0.00789943415530943,0.007899435725147804,0.007899380723114635,0.00789943551984691,0.009195346217420861,0.007899355085624744,0.007899449174895514,0.9276094089072194,0.007899431705714714]        |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004116156257648077,0.004116155360071098,0.0041161227955455555,0.0041161166027104375,0.004116130908599883,0.962660806514077,0.004116151591316547,0.004116157046936474,0.004410028534915029,0.004116174388179895]     |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.003640488416967432,0.003640469861347272,0.0036404593051187965,0.003640473018784207,0.0036404867695377383,0.9669756843416538,0.0036404925524208707,0.0036404890888178613,0.0039004531428145643,0.0036405035025375617]|\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.003944397155148947,0.003944373336963806,0.003944367754639512,0.0039443821549878825,0.003944430083133387,0.690260059479292,0.003944379806211201,0.003944388406596712,0.27818486445601515,0.003944357367011398]       |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.003640743725781927,0.003640729153285918,0.0036407050771173777,0.0036407107386943597,0.0036407338330933115,0.539314022647437,0.003640735885523001,0.00364072413899678,0.4315601526646818,0.0036407421353886623]      |\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.0037862888997282993,0.0037862726031487465,0.003786263554021164,0.0037862730791256003,0.00378629989498115,0.96565312028185,0.0037862988404481048,0.0037862900542531813,0.004056587492141197,0.0037863053003023913]   |\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004303556210872226,0.0043035256270398175,0.004303519579011586,0.004303541461084742,0.004303581180777133,0.8622595525745232,0.004303523473472376,0.004303530876383465,0.10331214458513036,0.004303524431705164]      |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.004303606124249909,0.00430360451228818,0.004303578702811412,0.004303551384573503,0.004303646982059587,0.005010678852765394,0.004303586956365435,0.004303625381662481,0.9605605303426057,0.004303590760618442]       |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.0032636320358430146,0.003263674245978077,0.003263656731967174,0.003263667889794804,0.0032636136645322273,0.6441461151778733,0.003263658821476355,0.0032636536259153695,0.32974465011303733,0.003263677693582205]    |\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.004116102959752695,0.004116069730055068,0.004116043862573044,0.00411605801468091,0.004116084419145459,0.9626609026273805,0.004116050396774687,0.004116076615325854,0.0044105415803209124,0.004116069793990791]      |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.004734999345859463,0.004734918628237146,0.004734864805585221,0.004734862172236349,0.0047349293175586345,0.005513522531491198,0.004734950619923468,0.0047349326246690976,0.9566070813145936,0.004734938639845805]    |\n",
      "+-----+---------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"D:\\spark231\\data\\mllib/sample_lda_libsvm_data.txt\")\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model (GMM)\n",
    "A Gaussian Mixture Model represents a composite distribution whereby points are drawn from one of k Gaussian sub-distributions, each with its own probability. The spark.ml implementation uses the expectation-maximization algorithm to induce the maximum-likelihood model given a set of samples.\n",
    "\n",
    "GaussianMixture is implemented as an Estimator and generates a GaussianMixtureModel as the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussians shown as a DataFrame: \n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|mean                                                         |cov                                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.10000000000001552,0.10000000000001552,0.10000000000001552]|0.006666666666806454  0.006666666666806454  0.006666666666806454  \n",
      "0.006666666666806454  0.006666666666806454  0.006666666666806454  \n",
      "0.006666666666806454  0.006666666666806454  0.006666666666806454  |\n",
      "|[9.099999999999984,9.099999999999984,9.099999999999984]      |0.006666666666812185  0.006666666666812185  0.006666666666812185  \n",
      "0.006666666666812185  0.006666666666812185  0.006666666666812185  \n",
      "0.006666666666812185  0.006666666666812185  0.006666666666812185  |\n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "# loads data\n",
    "dataset = spark.read.format(\"libsvm\").load(\"D:\\spark231\\data\\mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "gmm = GaussianMixture().setK(2).setSeed(538009335)\n",
    "model = gmm.fit(dataset)\n",
    "\n",
    "print(\"Gaussians shown as a DataFrame: \")\n",
    "model.gaussiansDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we load ratings data from the MovieLens dataset, each row consisting of a user, a movie, a rating and a timestamp. We then train an ALS model which assumes, by default, that the ratings are explicit (implicitPrefs is False). We evaluate the recommendation model by measuring the root-mean-square error of rating prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://github.com/apache/spark/blob/master/data/mllib/als/sample_movielens_ratings.txt'\n",
    "fileName = 'D:\\spark231\\data\\mllib\\sample_movielens_ratings.txt'\n",
    "req = requests.get(url)\n",
    "file = open(fileName, 'wb')\n",
    "for chunk in req.iter_content(100000):\n",
    "    file.write(chunk)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Pattern Mining\n",
    "Mining frequent items, itemsets, subsequences, or other substructures is usually among the first steps to analyze a large-scale dataset, which has been an active research topic in data mining for years. We refer users to Wikipedia’s association rule learning for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|    items|freq|\n",
      "+---------+----+\n",
      "|      [1]|   3|\n",
      "|      [2]|   3|\n",
      "|   [2, 1]|   3|\n",
      "|      [5]|   2|\n",
      "|   [5, 2]|   2|\n",
      "|[5, 2, 1]|   2|\n",
      "|   [5, 1]|   2|\n",
      "+---------+----+\n",
      "\n",
      "+----------+----------+------------------+\n",
      "|antecedent|consequent|        confidence|\n",
      "+----------+----------+------------------+\n",
      "|       [5]|       [2]|               1.0|\n",
      "|       [5]|       [1]|               1.0|\n",
      "|    [5, 1]|       [2]|               1.0|\n",
      "|    [5, 2]|       [1]|               1.0|\n",
      "|       [2]|       [1]|               1.0|\n",
      "|       [2]|       [5]|0.6666666666666666|\n",
      "|    [2, 1]|       [5]|0.6666666666666666|\n",
      "|       [1]|       [2]|               1.0|\n",
      "|       [1]|       [5]|0.6666666666666666|\n",
      "+----------+----------+------------------+\n",
      "\n",
      "+---+------------+----------+\n",
      "| id|       items|prediction|\n",
      "+---+------------+----------+\n",
      "|  0|   [1, 2, 5]|        []|\n",
      "|  1|[1, 2, 3, 5]|        []|\n",
      "|  2|      [1, 2]|       [5]|\n",
      "+---+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, [1, 2, 5]),\n",
    "    (1, [1, 2, 3, 5]),\n",
    "    (2, [1, 2])\n",
    "], [\"id\", \"items\"])\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
    "model = fpGrowth.fit(df)\n",
    "\n",
    "# Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "model.associationRules.show()\n",
    "\n",
    "# transform examines the input items against all the association rules and summarize the\n",
    "# consequents as prediction\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets. E.g., with k=3 folds, CrossValidator will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular ParamMap, CrossValidator computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs.\n",
    "\n",
    "After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.2581, 0.7419]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9186, 0.0814]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.432, 0.568]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.6766, 0.3234]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation Split\n",
    "In addition to CrossValidator Spark also offers TrainValidationSplit for hyper-parameter tuning. TrainValidationSplit only evaluates each combination of parameters once, as opposed to k times in the case of CrossValidator. It is therefore less expensive, but will not produce as reliable results when the training dataset is not sufficiently large.\n",
    "\n",
    "Unlike CrossValidator, TrainValidationSplit creates a single (training, test) dataset pair. It splits the dataset into these two parts using the trainRatio parameter. For example with trainRatio=0.75, TrainValidationSplit will generate a training and test dataset pair where 75% of the data is used for training and 25% for validation.\n",
    "\n",
    "Like CrossValidator, TrainValidationSplit finally fits the Estimator using the best ParamMap and the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            features|               label|          prediction|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|  -23.51088409032297| -1.6659388625179559|\n",
      "|(10,[0,1,2,3,4,5,...| -21.432387764165806|  0.3400877302576284|\n",
      "|(10,[0,1,2,3,4,5,...| -12.977848725392104|-0.02335359093652395|\n",
      "|(10,[0,1,2,3,4,5,...| -11.827072996392571|  2.5642684021108417|\n",
      "|(10,[0,1,2,3,4,5,...| -10.945919657782932| -0.1631314487734783|\n",
      "|(10,[0,1,2,3,4,5,...|  -10.58331129986813|   2.517790654691453|\n",
      "|(10,[0,1,2,3,4,5,...| -10.288657252388708| -0.9443474180536754|\n",
      "|(10,[0,1,2,3,4,5,...|  -8.822357870425154|  0.6872889429113783|\n",
      "|(10,[0,1,2,3,4,5,...|  -8.772667465932606|  -1.485408580416465|\n",
      "|(10,[0,1,2,3,4,5,...|  -8.605713514762092|   1.110272909026478|\n",
      "|(10,[0,1,2,3,4,5,...|  -6.544633229269576|  3.0454559778611285|\n",
      "|(10,[0,1,2,3,4,5,...|  -5.055293333055445|  0.6441174575094268|\n",
      "|(10,[0,1,2,3,4,5,...|  -5.039628433467326|  0.9572366607107066|\n",
      "|(10,[0,1,2,3,4,5,...|  -4.937258492902948|  0.2292114538379546|\n",
      "|(10,[0,1,2,3,4,5,...|  -3.741044592262687|   3.343205816009816|\n",
      "|(10,[0,1,2,3,4,5,...|  -3.731112242951253| -2.6826413698701064|\n",
      "|(10,[0,1,2,3,4,5,...|  -2.109441044710089| -2.1930034039595445|\n",
      "|(10,[0,1,2,3,4,5,...| -1.8722161156986976| 0.49547270330052423|\n",
      "|(10,[0,1,2,3,4,5,...| -1.1009750789589774| -0.9441633113006601|\n",
      "|(10,[0,1,2,3,4,5,...|-0.48115211266405217| -0.6756196573079968|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Prepare training and test data.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"D:\\spark231\\data\\mllib/sample_linear_regression_data.txt\")\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
